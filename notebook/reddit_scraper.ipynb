{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap reddit without pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '[D] Self-Promotion Thread', 'link': '/r/MachineLearning/comments/1j1hc0o/d_selfpromotion_thread/'}\n",
      "{'title': \"[D] Monthly Who's Hiring and Who wants to be Hired?\", 'link': '/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/'}\n",
      "{'title': '[R] Transformers without Normalization (FAIR Meta, New York University, MIT, Princeton University)', 'link': '/r/MachineLearning/comments/1jbs7xg/r_transformers_without_normalization_fair_meta/'}\n",
      "{'title': 'AirPods 4 with Active Noise Cancellation helps remove unwanted noise.', 'link': 'https://alb.reddit.com/cr?za=HK9Xb8uoAd9pkEmFi_WNR5_KzOzdMUiaLHibP_yl1qEcJWtJVhS3HMFDVOHtj_rEm2BXGX4YR938Ulxt9eLox9cmLvWR3jpeu_B34nWbeb1DzO1-6IIXAWfzc-ImRZpSRseHgGsL9l-7Xz20nlxOnQg3ueVooTdz8ZVUP_6G_anhrxb_7lqtg1UKkchhbyRvf7tGXHuT4IZtKVQorDKfEykKf7hhdbTAAYUEV1Knal_AANc4xg0ckO6jE-WKh1XbTSuAbZFr7AIY0Ukjm-Z8R30bZgE4QNpIRNqiG0Krhv1-l6s49mOvtOckeCD6EVw8anto1MvRJHE-zuqBi1ykPPwkuWc9Cg81UnK7JSn0rTS67jGMrkmmaSWPmcPJWq9vdqz0V1R9gqsfvp10udo-cPySXlb2YDkA-TXEOCnVVc5PLFa9lTKMTrLE9gFqLIMJj1LVPH7byEMhCle07WGnwkzw5xp4CAp5aNoco7s21TehMXRIOtWin3ZtIl6_mrPTblXqw7HFUb7qmwqI8eeMjNBYSHNJfFZaOshrnOp7hoPO_Y1ZaSSIs_lwkVSF&zp=KOHCP6qAvlAT0OD0LNg6YK2VzWRiro-CX4vwKcwJ65iZ90zNEa37j9oukvbLvb69Yrp2V-4SVCUgHRiZK_BeTfs6Aen3nJbdBzs3QDZJddvqwDpiFTKqMciKVf_8yQDa-aWGPg3V8Yex7cMhJoq5wtFlSzRg99ZR0Rff9wnTpMlQ_DLBp7SEjeRiKd2oSw-1cIMPBolZr8lEejdwYuEo-RxkEdrN10_P-Cwo_q5LCZxr6NxpfNVFYEZenv2IoOqt8IS-BlMAI6sEvH95E10MKCDJ-aF7SINMXRHk9oXk_w2rrJ6svYVnyio1HNmKhqW5UqS1Rz6Zdrn1LtnM0fCumbUySGjgA7XqeDsfbs2uUS7a5LhRGqsQibPWyezoh1T3Qlh_UaXsMYQDafOueUypF16k8xouzUG3Sk-bgcqrDD-U5DB5axA'}\n",
      "{'title': '[P] New Python library for axis labeling algorithms', 'link': '/r/MachineLearning/comments/1jchg8d/p_new_python_library_for_axis_labeling_algorithms/'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the subreddit\n",
    "subreddit = \"machinelearning\"\n",
    "url = f\"https://old.reddit.com/r/{subreddit}/\"\n",
    "\n",
    "# Headers to mimic a browser\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# Request the page\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Parse the HTML\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Extract post titles and links\n",
    "posts = []\n",
    "for post in soup.find_all(\"div\", class_=\"thing\"):\n",
    "    title = post.find(\"a\", class_=\"title\").text\n",
    "    link = post.find(\"a\", class_=\"title\")[\"href\"]\n",
    "    posts.append({\"title\": title, \"link\": link})\n",
    "\n",
    "# Print results\n",
    "for p in posts[:5]:\n",
    "    print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap reddit with pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '[D] Self-Promotion Thread', 'link': '/r/MachineLearning/comments/1j1hc0o/d_selfpromotion_thread/'}\n",
      "{'title': \"[D] Monthly Who's Hiring and Who wants to be Hired?\", 'link': '/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/'}\n",
      "{'title': '[P] New Python library for axis labeling algorithms', 'link': '/r/MachineLearning/comments/1jchg8d/p_new_python_library_for_axis_labeling_algorithms/'}\n",
      "{'title': 'Meet iPhone 16e. Built for Apple Intelligence and powered by A18 â€” the latest-generation chip â€” it comes packed with a 48MP Fusion camera, supersized battery life and a durable design.', 'link': 'https://alb.reddit.com/cr?za=9sQ0BUkSwF6i_aWlpRv9V3zluEavx5F9SYTztsKlBvOO12C6lQHPeRCwXsDCehZgVU4kIdekEE-KFjtJga4dqU_UvOTRhrPMb8XgA9h4wmWObY1gus9F1NnUvxKoWvcHWHQC2o4obIypXqSugqarS_TRp3OEpM0r05EbkVHNJvSEcf7rEjDAeAiXBdDFG_kH0KhfA_zQA6R6ualxfpgPPfUMK7pHTK1iKmKz8zlpvKgHuddkiPDISQQ9hROHJZhWw7u1soe_E5yC1KdQ4cPbLyFhmjZ5c8UeUe7UEsWFf_7T5le-kh_8vCgCykIUARPuoNx0jaufp8zCTEAfs7BdxzPUBrZJfE55j2BypqViUmfwFy7xqljlKuwuCoWjprkCj5tumBZ6FUMJrZ0pe72Zvb9PVkkRhiyoa6W9qOZLsJVQ2IOjpDF9q6AF5OdMk9N4bgWenIFeMZ8gMInrZlRL_XQXqZsiVtUx6ycd6pDTeO_8QOmg0VyxYWpbOwdOegxHZH4SCxMafdhuIUrosfvTZ11jk8Z8y47RXTjUHddTa2nlLljgcCQpcoU&zp=nkjRi1tUC3F-RyKuGz0ymGt2CIhHRrlrW8h8f4tg9H-LRggCTGMPwS47MPSy5VrdX3SvWvBnuXX6Hp270TjjV6JowB3w0qBZl4l2AbAGW0MBD52IRBv6oknrqPRvVqOe2YSzUW0WYDOhl4PFMgHwmXLuIlzrRltLpeX86tFq5BaGVct1OgUxm_bWNxQ6PGxFsqaZTpzml4OrsC_9dXw_G-5_3lLvX5s5YKmZghmp01eS1GI0usf_25LtZpCUPGxAMPdcg2Ii5--R-QpXJkpc-fBFoSo5mgN6ZsId-ezu9Hyanj-K4ISWjsW6cD1FAZHplhMMRvTozc1Oi3iS6G9xFAtENzHKfjnAH5tjCY1RjPf30OUHNv0B9LJnDv5SkWDn5ipjhKcYR26hRq_hJzNRne096KoWBM8Hyh_vc0XuJOTXDjg-mMj_mFU'}\n",
      "{'title': '[R] Transformers without Normalization (FAIR Meta, New York University, MIT, Princeton University)', 'link': '/r/MachineLearning/comments/1jbs7xg/r_transformers_without_normalization_fair_meta/'}\n",
      "{'title': '[P] Insights from Building an Embeddings and Retrieval-Augmented Generation App from scratch', 'link': 'https://amritpandey23.github.io/posts/2024/11/insights-from-embeddings-and-retrieval-augmented-generation/'}\n",
      "{'title': '[D] The Cultural Divide between Mathematics and AI', 'link': 'https://sugaku.net/content/understanding-the-cultural-divide-between-mathematics-and-ai/'}\n",
      "{'title': '[R] 4D Language Fields for Dynamic Scenes via MLLM-Guided Object-wise Video Captioning', 'link': '/r/MachineLearning/comments/1jcfw3p/r_4d_language_fields_for_dynamic_scenes_via/'}\n",
      "{'title': '[R] Recent advances in recurrent neural networks---any sleepers?', 'link': '/r/MachineLearning/comments/1jbzcoc/r_recent_advances_in_recurrent_neural_networksany/'}\n",
      "{'title': '[D] Confidence score behavior for object detection models', 'link': '/r/MachineLearning/comments/1jc8icd/d_confidence_score_behavior_for_object_detection/'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_reddit(subreddit, pages=2):\n",
    "    base_url = f\"https://old.reddit.com/r/{subreddit}/\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    next_page = base_url\n",
    "    all_posts = []\n",
    "\n",
    "    for _ in range(pages):\n",
    "        response = requests.get(next_page, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(\"Failed to fetch data\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Extract posts\n",
    "        for post in soup.find_all(\"div\", class_=\"thing\"):\n",
    "            title = post.find(\"a\", class_=\"title\").text\n",
    "            link = post.find(\"a\", class_=\"title\")[\"href\"]\n",
    "            all_posts.append({\"title\": title, \"link\": link})\n",
    "\n",
    "        # Find the next page button\n",
    "        next_button = soup.find(\"span\", class_=\"next-button\")\n",
    "        if next_button:\n",
    "            next_page = next_button.find(\"a\")[\"href\"]\n",
    "        else:\n",
    "            break  # No more pages\n",
    "\n",
    "    return all_posts\n",
    "\n",
    "# Usage\n",
    "subreddit = \"machinelearning\"\n",
    "posts = scrape_reddit(subreddit, pages=2)\n",
    "\n",
    "# Print first 10 posts\n",
    "for p in posts[:10]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '[D] Could an AI Model Truly Evolve Beyond Predefined Learning?',\n",
       " 'link': '/r/MachineLearning/comments/1jaf1xm/d_could_an_ai_model_truly_evolve_beyond/'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap with pagination and data [title, content, date, comment, vote]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def scrape_reddit(subreddit, pages=1):\n",
    "    base_url = f\"https://old.reddit.com/r/{subreddit}/\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    next_page = base_url\n",
    "    all_posts = []\n",
    "\n",
    "    for _ in range(pages):\n",
    "        response = requests.get(next_page, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(\"Failed to fetch data\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Extract posts\n",
    "        for post in soup.find_all(\"div\", class_=\"thing\"):\n",
    "            title = post.find(\"a\", class_=\"title\").text\n",
    "            link = post.find(\"a\", class_=\"title\")[\"href\"]\n",
    "            full_link = urljoin(base_url, link)  # Ensures absolute URL\n",
    "            votes = post.find(\"div\", class_=\"score unvoted\")\n",
    "            votes = votes.text if votes else \"N/A\"\n",
    "            comments = post.find(\"a\", string=lambda text: text and \"comment\" in text.lower())\n",
    "            comments = comments.text if comments else \"0 comments\"\n",
    "            author = post.find(\"a\", class_=\"author\")\n",
    "            author = author.text if author else \"Unknown\"\n",
    "            date = post.find(\"time\")\n",
    "            date = date[\"datetime\"] if date else \"Unknown\"\n",
    "\n",
    "            all_posts.append({\n",
    "                \"title\": title,\n",
    "                \"link\": full_link,\n",
    "                \"votes\": votes,\n",
    "                \"comments\": comments,\n",
    "                \"author\": author,\n",
    "                \"date\": date\n",
    "            })\n",
    "\n",
    "        # Find the next page button\n",
    "        next_button = soup.find(\"span\", class_=\"next-button\")\n",
    "        if next_button:\n",
    "            next_page = next_button.find(\"a\")[\"href\"]\n",
    "        else:\n",
    "            break  # No more pages\n",
    "\n",
    "    return all_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "POST_LIMIT = 500\n",
    "subreddit_list = [\n",
    "    \"r/india\",  \n",
    "    \"r/IndianGaming\",  \n",
    "    \"r/IndianFood\",  \n",
    "    \"r/desis\",  \n",
    "    \"r/IndiaSpeaks\",  \n",
    "    \"r/bollywood\",  \n",
    "    \"r/IndianMusic\",  \n",
    "    \"r/IndianFashionAddicts\",  \n",
    "    \"r/IndianPeopleFacebook\",  \n",
    "    \"r/AskIndia\",  \n",
    "    \"r/IndianDiaspora\",  \n",
    "    \"r/Sikh\",  \n",
    "    \"r/hindustan\",  \n",
    "    \"r/TwoXIndia\",  \n",
    "    \"r/Chennai\",  \n",
    "    \"r/Bangalore\",  \n",
    "    \"r/Mumbai\",  \n",
    "    \"r/Kolkata\",  \n",
    "    \"r/delhi\",  \n",
    "    \"r/indiauncensored\",  \n",
    "    \"r/IndiaInvestments\",  \n",
    "    \"r/IndianArt\",  \n",
    "    \"r/IndianProgramming\",  \n",
    "    \"r/SouthAsianFood\",  \n",
    "    \"r/IndianFootball\",  \n",
    "    \"r/IndianMusicExchange\",  \n",
    "    \"r/indiadiscussion\",  \n",
    "    \"r/IndiaSocial\",  \n",
    "    \"r/IndianMemes\",  \n",
    "    \"r/IndianHistory\",  \n",
    "    \"r/IndianPolitics\",  \n",
    "    \"r/Cricket\",  \n",
    "    \"r/IndianStockMarket\",  \n",
    "    \"r/HindutvaWatch\",  \n",
    "    \"r/NorthEastIndia\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = scrape_reddit(\"india\", pages=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_post = []\n",
    "\n",
    "page_number = 1\n",
    "while len(total_post) < 10:\n",
    "    posts = scrape_reddit(\"india\", pages=page_number)\n",
    "    total_post.extend(posts)\n",
    "    page_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Surrender India Passport in India : Pune Regional passport office',\n",
       " 'link': 'https://old.reddit.com/r/india/comments/1jdvlm6/surrender_india_passport_in_india_pune_regional/',\n",
       " 'votes': '221',\n",
       " 'comments': '27 comments',\n",
       " 'author': 'jayrohi18',\n",
       " 'date': '2025-03-18T03:07:56+00:00'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_post[-10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sqlite program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from hashlib import sha256\n",
    "from datetime import datetime\n",
    "\n",
    "DB_NAME = \"reddit_posts.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Œ Create SQLite Database and Table\n",
    "def create_database():\n",
    "    conn = sqlite3.connect(DB_NAME)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS posts (\n",
    "            title TEXT,\n",
    "            link TEXT,\n",
    "            votes INTEGER,\n",
    "            comments INTEGER,\n",
    "            author TEXT,\n",
    "            date TEXT,\n",
    "            age INTEGER,\n",
    "            hash TEXT UNIQUE\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Œ Insert a Post into the Database (Ensuring Uniqueness)\n",
    "def insert_post(title, link, votes, comments, author, date):\n",
    "    conn = sqlite3.connect(DB_NAME)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Convert votes and comments to integers\n",
    "    votes = int(votes) if votes.isdigit() else 0\n",
    "    comments = int(comments.split()[0]) if comments.split()[0].isdigit() else 0\n",
    "    \n",
    "    # Calculate Age (days since post)\n",
    "    post_date = datetime.strptime(date, \"%Y-%m-%dT%H:%M:%S\") if date != \"Unknown\" else datetime.now()\n",
    "    scrape_time = datetime.now()\n",
    "\n",
    "    # Generate Unique Hash (SHA256 of Title)\n",
    "    post_hash = sha256(title.encode()).hexdigest()\n",
    "\n",
    "    try:\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO posts (title, link, votes, comments, author, date, scrape_time, hash)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\", (title, link, votes, comments, author, date, scrape_time, post_hash))\n",
    "        conn.commit()\n",
    "    except sqlite3.IntegrityError:\n",
    "        print(f\"Skipping duplicate post: {title}\")\n",
    "\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Œ Search Posts by Title Keyword\n",
    "def search_posts(keyword):\n",
    "    conn = sqlite3.connect(DB_NAME)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(\"SELECT * FROM posts WHERE title LIKE ?\", ('%' + keyword + '%',))\n",
    "    results = cursor.fetchall()\n",
    "    \n",
    "    conn.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Œ Scrape and Store Posts in Database\n",
    "def scrape_reddit(subreddit, pages=1):\n",
    "    base_url = f\"https://old.reddit.com/r/{subreddit}/\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    next_page = base_url\n",
    "\n",
    "    for _ in range(pages):\n",
    "        response = requests.get(next_page, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(\"Failed to fetch data\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Extract posts\n",
    "        for post in soup.find_all(\"div\", class_=\"thing\"):\n",
    "            title = post.find(\"a\", class_=\"title\").text\n",
    "            link = urljoin(base_url, post.find(\"a\", class_=\"title\")[\"href\"])\n",
    "            votes = post.find(\"div\", class_=\"score unvoted\")\n",
    "            votes = votes.text if votes else \"0\"\n",
    "            comments = post.find(\"a\", string=lambda text: text and \"comment\" in text.lower())\n",
    "            comments = comments.text if comments else \"0 comments\"\n",
    "            author = post.find(\"a\", class_=\"author\")\n",
    "            author = author.text if author else \"Unknown\"\n",
    "            date = post.find(\"time\")[\"datetime\"] if post.find(\"time\") else \"Unknown\"\n",
    "\n",
    "            # Insert into database\n",
    "            insert_post(title, link, votes, comments, author, date)\n",
    "\n",
    "        # Find the next page button\n",
    "        next_button = soup.find(\"span\", class_=\"next-button\")\n",
    "        if next_button:\n",
    "            next_page = next_button.find(\"a\")[\"href\"]\n",
    "        else:\n",
    "            break  # No more pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
