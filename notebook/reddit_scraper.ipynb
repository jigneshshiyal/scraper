{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap reddit without pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '[D] Self-Promotion Thread', 'link': '/r/MachineLearning/comments/1j1hc0o/d_selfpromotion_thread/'}\n",
      "{'title': \"[D] Monthly Who's Hiring and Who wants to be Hired?\", 'link': '/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/'}\n",
      "{'title': '[R] SEA-VL: A Large-Scale Culturally-Relevant Vision-Language Dataset for Southeast Asian Languages', 'link': '/r/MachineLearning/comments/1ja9stg/r_seavl_a_largescale_culturallyrelevant/'}\n",
      "{'title': 'AirPods 4 with Active Noise Cancellation helps remove unwanted noise.', 'link': 'https://alb.reddit.com/cr?za=YiALcnXTDSEB2A1tofiXUDcK9gYTbE-0_vtQt1IKX8Kspoo_5h6r5taJ8cqYzvf11Z9MoPgHR_8Odt8e6idsOw19Y_bW6CRf_uccnagIBlu4QiaBYu_XmPUqpiYiRfME5lPYFPupWGTGAucPLWuFi58fwp1YsKQNVPWoI9Hu3m2TEKk8kgqKd8YVXkWW5_NcKo4jZeuUfuDzmDEzKzbVgzEnj8Ew7zpY6sTwWITZs1BhALUckF5oBKjySHECPNisuZZ0tTuf-a149_OGETFNBv4QXFqA2UCGNl-LzJTgTv7Hg8FEiEMRqA1DxTK60fmVXl83TuX4_oSUsSdarCJhfYTX_LPZnbjhNWC13GVohXjKMYjUBCwRtx_4rfWmUCH-bCUP0GqsXFMEzdHSuwT90sf9EgFJYLQSfXOVQDwe5dllOTKsoIkt6NVR328b7drxtG1pKZNRPnCrDdMKxRV2XSKHZ2ul68owzvK2sIZ2vvfxEi9Tqu1_bNWTqK2X0EWc3SZ2gp_OZhh2Uous_PsD0YVfMBqy4vA0t0zROm7Aayl744au7kjXQg&zp=KQviMuckPEevrzea8D0PODx34urCYOaKbSYqCNA5pZQ0KmDsOZ_OT8HHOu07dDCfZunBE-aYyV-dawUQXiHfXCfbOrJervgsQ-4p0hQ93Dz11DJiFE0JRO_EqpINo0Hr8JOLC5fHjJjjVtnvizIEGlDp4RtUIfXoaVFMD7LRBT1hp8vmMrru0WQS8eMX45BUkQAUT6Beo0cLs3x0_gap96HAl1yYWvl16ZEBiEPOvRdwBXzWt6IZ0Aqfo_rE4rQK41rdBLcmHvaKWNpwsnvrQK-Zn6HmatAXgiEe0IEvXRSKoaJA1oeDCvQz_iXd8AWjQMPdmpZ0IEEGoQmPkxcZ9PjCyuenZE6yh7PKSLeJELF8YyNwdyrZX4h9S8aGWevCGMGUhxBVx_Vn0KRsaAv55mdAUQx2zHgW8hNxt8HqIRNH3hIE7MQ'}\n",
      "{'title': 'Gemma 3 released: beats Deepseek v3 in the Arena, while using 1 GPU instead of 32 [N]', 'link': '/r/MachineLearning/comments/1j9npsl/gemma_3_released_beats_deepseek_v3_in_the_arena/'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the subreddit\n",
    "subreddit = \"machinelearning\"\n",
    "url = f\"https://old.reddit.com/r/{subreddit}/\"\n",
    "\n",
    "# Headers to mimic a browser\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# Request the page\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Parse the HTML\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Extract post titles and links\n",
    "posts = []\n",
    "for post in soup.find_all(\"div\", class_=\"thing\"):\n",
    "    title = post.find(\"a\", class_=\"title\").text\n",
    "    link = post.find(\"a\", class_=\"title\")[\"href\"]\n",
    "    posts.append({\"title\": title, \"link\": link})\n",
    "\n",
    "# Print results\n",
    "for p in posts[:5]:\n",
    "    print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap reddit with pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '[D] Self-Promotion Thread', 'link': '/r/MachineLearning/comments/1j1hc0o/d_selfpromotion_thread/'}\n",
      "{'title': \"[D] Monthly Who's Hiring and Who wants to be Hired?\", 'link': '/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/'}\n",
      "{'title': '[R] SEA-VL: A Large-Scale Culturally-Relevant Vision-Language Dataset for Southeast Asian Languages', 'link': '/r/MachineLearning/comments/1ja9stg/r_seavl_a_largescale_culturallyrelevant/'}\n",
      "{'title': 'Gamers on Reddit rely on Reddit for trustworthy information. Plus, theyâ€™re 27% more likely to buy products they see advertised.', 'link': 'https://alb.reddit.com/cr?za=HCHlQ7lZjpfIo8i5IC0TpWfWvnaqfhFYV5tKq1pmafv5r7hFz08uKkDIKdBL4OjceaXUYrauq8lT8eJ8BQaJ0bn2AMJpZYL4_a7oR8qmAK0g0oIkhwsc7jihAm4Fkj_TnbID8ydDZ9nwKlb-Zl2hSowfbaWYmppYuwR3meFjrleXgzaH5jGcZEOwBxzFK-Pto-jaLS7Fsbb3q3OeC9_a6E3tLHYNGgVgn8tsTPFU8RDrvfdWVpVTVhRIUqn9EDsWHYuMBWEySiT3qC5QD5utSHJemr9wFjwV0R2XM_K4iAGHtevUzfvKjVrbMeqziQ0GmPRFnOLiIbjBQnV72cBrlYsXAahAh8oqUhVzlVHOSDMDNvL9oJMFZnPz8t0Y8m2sHVECVDxPvJWaVwrih9LUWc_2BtlJiT7GN91QJmjbP-Km-E7tNwfFI0D8yckJb1bG3vMz5JrrDzlCOPBnVC7R-4g81Cfegs3qOlVlmmRAcz-S6rQn-ZqJcawwvhHMxbns-iDTL14cOpBJRna8AFb7nrfrI1Y8&zp=6g8L1L-w3qxzbSv1sW9adKa4Uevp3UCiuhPKiwaxAPyps55tpDt23L_uBf_3tOeyABetqg--N6YErBSW5S6xl-PdR1GIHxlohvwVjM89yXbgtLckkO60pMY-yXngHEpelY6cUAapeYZfQ47Yr51B91udCi0KGOOKyGU9oRydhv8JRD9MZma1IvhZsNY5VRBfUOE2ZfsPDtPOrpBEebOvSR0REhBOiNPtiulyEe_7HY8Z9-u7swpvL_rEZBfKZTBvj9jJw3RQwN0o32X2EmfPr3ZD3D0CxlfC0t6MGfpgZnjz3vfBWpqxDFgb8OtShnTWlmfNMUUPNVn0Q2QvNOUSZ6k38ZVaPdOspU4_ZW4eOqMnHbdKjjg_QhjrPFPFWqW6P0ykwF7J1-gqFqq-EonOLFAfkSMIMJeKtKTvpjZDCIuk1HMYokscj888YH0QBjQXvzlHvYBBWirFtA99bKV_0C_bQ75nJ-IkujLkRMvzzw'}\n",
      "{'title': 'Gemma 3 released: beats Deepseek v3 in the Arena, while using 1 GPU instead of 32 [N]', 'link': '/r/MachineLearning/comments/1j9npsl/gemma_3_released_beats_deepseek_v3_in_the_arena/'}\n",
      "{'title': \"[D] Geometric Deep learning and it's potential\", 'link': '/r/MachineLearning/comments/1jabkt8/d_geometric_deep_learning_and_its_potential/'}\n",
      "{'title': '[D]Good resources/papers for understanding image2video diffusion models', 'link': '/r/MachineLearning/comments/1ja12f3/dgood_resourcespapers_for_understanding/'}\n",
      "{'title': '[D] Any IEEE Transactions where I can submit', 'link': '/r/MachineLearning/comments/1ja9z1s/d_any_ieee_transactions_where_i_can_submit/'}\n",
      "{'title': '[R] Are there new advance types of llm architecture in reasearch/production?', 'link': '/r/MachineLearning/comments/1ja59uf/r_are_there_new_advance_types_of_llm_architecture/'}\n",
      "{'title': '[D] ICLR Camera ready: remove anonymous code?', 'link': '/r/MachineLearning/comments/1ja2qm9/d_iclr_camera_ready_remove_anonymous_code/'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_reddit(subreddit, pages=2):\n",
    "    base_url = f\"https://old.reddit.com/r/{subreddit}/\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    next_page = base_url\n",
    "    all_posts = []\n",
    "\n",
    "    for _ in range(pages):\n",
    "        response = requests.get(next_page, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(\"Failed to fetch data\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Extract posts\n",
    "        for post in soup.find_all(\"div\", class_=\"thing\"):\n",
    "            title = post.find(\"a\", class_=\"title\").text\n",
    "            link = post.find(\"a\", class_=\"title\")[\"href\"]\n",
    "            all_posts.append({\"title\": title, \"link\": link})\n",
    "\n",
    "        # Find the next page button\n",
    "        next_button = soup.find(\"span\", class_=\"next-button\")\n",
    "        if next_button:\n",
    "            next_page = next_button.find(\"a\")[\"href\"]\n",
    "        else:\n",
    "            break  # No more pages\n",
    "\n",
    "    return all_posts\n",
    "\n",
    "# Usage\n",
    "subreddit = \"machinelearning\"\n",
    "posts = scrape_reddit(subreddit, pages=2)\n",
    "\n",
    "# Print first 10 posts\n",
    "for p in posts[:10]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap with pagination and data [title, content, date, comment, vote]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_reddit(subreddit, pages=1):\n",
    "    base_url = f\"https://old.reddit.com/r/{subreddit}/\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    next_page = base_url\n",
    "    all_posts = []\n",
    "\n",
    "    for _ in range(pages):\n",
    "        response = requests.get(next_page, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(\"Failed to fetch data\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Extract posts\n",
    "        for post in soup.find_all(\"div\", class_=\"thing\"):\n",
    "            title = post.find(\"a\", class_=\"title\").text\n",
    "            link = post.find(\"a\", class_=\"title\")[\"href\"]\n",
    "            votes = post.find(\"div\", class_=\"score unvoted\")\n",
    "            votes = votes.text if votes else \"N/A\"\n",
    "            comments = post.find(\"a\", string=lambda text: text and \"comment\" in text.lower())\n",
    "            comments = comments.text if comments else \"0 comments\"\n",
    "            author = post.find(\"a\", class_=\"author\")\n",
    "            author = author.text if author else \"Unknown\"\n",
    "            date = post.find(\"time\")\n",
    "            date = date[\"datetime\"] if date else \"Unknown\"\n",
    "\n",
    "            all_posts.append({\n",
    "                \"title\": title,\n",
    "                \"link\": link,\n",
    "                \"votes\": votes,\n",
    "                \"comments\": comments,\n",
    "                \"author\": author,\n",
    "                \"date\": date\n",
    "            })\n",
    "\n",
    "        # Find the next page button\n",
    "        next_button = soup.find(\"span\", class_=\"next-button\")\n",
    "        if next_button:\n",
    "            next_page = next_button.find(\"a\")[\"href\"]\n",
    "        else:\n",
    "            break  # No more pages\n",
    "\n",
    "    return all_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POST_LIMIT = 500\n",
    "subreddit_list = indian_subreddits = [\n",
    "    \"r/india\",  \n",
    "    \"r/IndianGaming\",  \n",
    "    \"r/IndianFood\",  \n",
    "    \"r/desis\",  \n",
    "    \"r/IndiaSpeaks\",  \n",
    "    \"r/bollywood\",  \n",
    "    \"r/IndianMusic\",  \n",
    "    \"r/IndianFashionAddicts\",  \n",
    "    \"r/IndianPeopleFacebook\",  \n",
    "    \"r/AskIndia\",  \n",
    "    \"r/IndianDiaspora\",  \n",
    "    \"r/Sikh\",  \n",
    "    \"r/hindustan\",  \n",
    "    \"r/TwoXIndia\",  \n",
    "    \"r/Chennai\",  \n",
    "    \"r/Bangalore\",  \n",
    "    \"r/Mumbai\",  \n",
    "    \"r/Kolkata\",  \n",
    "    \"r/delhi\",  \n",
    "    \"r/indiauncensored\",  \n",
    "    \"r/IndiaInvestments\",  \n",
    "    \"r/IndianArt\",  \n",
    "    \"r/IndianProgramming\",  \n",
    "    \"r/SouthAsianFood\",  \n",
    "    \"r/IndianFootball\",  \n",
    "    \"r/IndianMusicExchange\",  \n",
    "    \"r/indiadiscussion\",  \n",
    "    \"r/IndiaSocial\",  \n",
    "    \"r/IndianMemes\",  \n",
    "    \"r/IndianHistory\",  \n",
    "    \"r/IndianPolitics\",  \n",
    "    \"r/Cricket\",  \n",
    "    \"r/IndianStockMarket\",  \n",
    "    \"r/HindutvaWatch\",  \n",
    "    \"r/NorthEastIndia\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
